{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marianaborralho/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marianaborralho/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marianaborralho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/marianaborralho/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bibliotecas\n",
    "\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from spacy.tokens import Doc\n",
    "import stanza\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.utils import resample\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import random\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Análise de Sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dados\n",
    "\n",
    "amazon_reviews: Conjunto não balanceado, com cerca de 50000 reviews de produtos da empresa Amazon, anotadas com as etiquetas “positive” e “negative”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar os dados de treino\n",
    "train_data = pd.read_csv(\"amazon_reviews_train.csv\")\n",
    "\n",
    "# Carregar os dados de teste\n",
    "test_data = pd.read_csv(\"amazon_reviews_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48902, 2)\n",
      "(2417, 2)\n"
     ]
    }
   ],
   "source": [
    "# Verificar o tamanho dos dados\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values: 0\n",
      "NA values: False\n",
      "Null values: 0\n",
      "NA values: False\n"
     ]
    }
   ],
   "source": [
    "# Verificar os NA no conjunto de treino\n",
    "print(\"Null values:\",train_data.isnull().values.sum())\n",
    "print(\"NA values:\", train_data.isna().values.any())\n",
    "\n",
    "\n",
    "# Verificar os NA no conjunto de teste\n",
    "print(\"Null values:\",test_data.isnull().values.sum())\n",
    "print(\"NA values:\", test_data.isna().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             review\n",
       "0  negative  Product arrived labeled as Jumbo Salted Peanut...\n",
       "1  positive  This is a confection that has been around a fe...\n",
       "2  negative  If you are looking for the secret ingredient i...\n",
       "3  positive  Great taffy at a great price.  There was a wid...\n",
       "4  positive  This saltwater taffy had great flavors and was..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in training set: 48902\n",
      "Number of documents in test set: 2417\n",
      "\n",
      "Label distribution in training set:\n",
      "positive    37835\n",
      "negative    11067\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Label distribution in test set:\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Número de documentos\n",
    "num_docs_train = len(train_data)\n",
    "num_docs_test = len(test_data)\n",
    "\n",
    "print(f\"Number of documents in training set: {num_docs_train}\")\n",
    "print(f\"Number of documents in test set: {num_docs_test}\")\n",
    "\n",
    "# Distribuição de etiquetas\n",
    "label_distribution_train = train_data['sentiment'].value_counts()\n",
    "label_distribution_test = test_data['sentiment'].value_counts()\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "print(label_distribution_train)\n",
    "\n",
    "print(\"\\nLabel distribution in test set:\")\n",
    "print(label_distribution_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = sns.color_palette('deep')\n",
    "\n",
    "plt.figure(figsize=(6,4), tight_layout=True)\n",
    "plt.bar(x=['Positive', 'Negative'],\n",
    "        height=train_data['sentiment'].value_counts(),\n",
    "        color=colors[:2])\n",
    "plt.title('Sentiment in Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de treino não balanceado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive    37835\n",
      "negative    11067\n",
      "Name: sentiment, dtype: int64\n",
      "negative    11067\n",
      "positive    11067\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Configura o undersampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Separa os dados e as etiquetas\n",
    "X_train = train_data[['review']]\n",
    "y_train = train_data['sentiment']\n",
    "\n",
    "# Aplica o undersampling\n",
    "X_res, y_res = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Verifica o novo balanceamento das classes\n",
    "#print(pd.Series(y_res).value_counts())\n",
    "\n",
    "# Criar um novo dataframe com os dados balanceados\n",
    "train_data_balanced = pd.concat([X_res, pd.Series(y_res, name='sentiment')], axis=1)\n",
    "\n",
    "print(train_data['sentiment'].value_counts())\n",
    "print(train_data_balanced['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Definição de um baseline usando ferramentas já existentes\n",
    "\n",
    "- TextBlob\n",
    "- Vader Sentiment\n",
    "- Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar biblioteca textblob\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar biblioteca vaderSentiment\n",
    "# !pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar biblioteca stanza\n",
    "# !pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment', 'review'], dtype='object')\n",
      "1    1676\n",
      "0     741\n",
      "Name: sentiment, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.36      0.48       741\n",
      "           1       0.77      0.94      0.85      1676\n",
      "\n",
      "    accuracy                           0.76      2417\n",
      "   macro avg       0.75      0.65      0.67      2417\n",
      "weighted avg       0.76      0.76      0.74      2417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Verificar os nomes das colunas\n",
    "print(test_data.columns)\n",
    "\n",
    "\n",
    "# Função para prever o sentimento com TextBlob\n",
    "def predict_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    # TextBlob retorna polaridade dentro do intervalo [-1, 1], onde valores positivos indicam sentimentos positivos\n",
    "    return \"positive\" if analysis.sentiment.polarity >= 0 else \"negative\"\n",
    "\n",
    "\n",
    "# Aplicar a função de previsão de sentimento aos dados de teste\n",
    "test_data[\"predicted_sentiment_TextBlob\"] = test_data[\"review\"].apply(predict_sentiment)\n",
    "\n",
    "# Converter rótulos de string para binários\n",
    "test_data[\"sentiment\"] = test_data[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "test_data[\"predicted_sentiment_TextBlob\"] = test_data[\"predicted_sentiment_TextBlob\"].map({\"positive\": 1, \"negative\": 0})\n",
    "\n",
    "print(test_data[\"sentiment\"].value_counts())\n",
    "\n",
    "# Calcular as métricas de avaliação\n",
    "print(classification_report(test_data[\"sentiment\"], test_data[\"predicted_sentiment_TextBlob\"] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.39      0.53       741\n",
      "           1       0.78      0.96      0.86      1676\n",
      "\n",
      "    accuracy                           0.79      2417\n",
      "   macro avg       0.80      0.68      0.70      2417\n",
      "weighted avg       0.79      0.79      0.76      2417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Inicializar o analisador de sentimentos VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "# Função para prever o sentimento com VADER\n",
    "def predict_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return \"positive\" if scores[\"compound\"] >= 0 else \"negative\"\n",
    "\n",
    "\n",
    "# Aplicar a função de previsão de sentimento aos dados de teste\n",
    "test_data[\"predicted_sentiment_VADER\"] = test_data[\"review\"].apply(predict_sentiment)\n",
    "\n",
    "# Converter rótulos de string para binários\n",
    "test_data[\"predicted_sentiment_VADER\"] = test_data[\"predicted_sentiment_VADER\"].map({\"positive\": 1, \"negative\": 0})\n",
    "\n",
    "print(classification_report(test_data[\"sentiment\"], test_data[\"predicted_sentiment_VADER\"] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import stanza\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# # Inicializar o pipeline NLP Stanza\n",
    "# nlp = stanza.Pipeline(\"en\", processors=\"tokenize,sentiment\")\n",
    "\n",
    "\n",
    "# # Função para prever o sentimento com Stanza\n",
    "# def predict_sentiment(text):\n",
    "#     doc = nlp(text)\n",
    "#     sentiment_scores = [sentence.sentiment for sentence in doc.sentences]\n",
    "#     avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
    "#     return \"positive\" if avg_sentiment >= 1 else \"negative\"\n",
    "\n",
    "\n",
    "# # Aplicar a função de previsão de sentimento aos dados de teste\n",
    "# test_data[\"predicted_sentiment_Stanza\"] = test_data[\"review\"].apply(predict_sentiment)\n",
    "\n",
    "# # Converter rótulos de string para binários\n",
    "# test_data[\"predicted_sentiment_Stanza\"] = test_data[\"predicted_sentiment_Stanza\"].map(\n",
    "#     {\"positive\": 1, \"negative\": 0}\n",
    "# )\n",
    "\n",
    "# # Calcular as métricas de avaliação\n",
    "# print(classification_report(test_data[\"sentiment\"], test_data[\"predicted_sentiment_Stanza\"] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preparação de dados e aplicação de um léxico de sentimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Léxicos de polaridade** são recursos valiosos em processamento de linguagem natural (NLP), particularmente úteis para tarefas como análise de sentimentos, onde o objetivo é determinar a atitude ou emoção expressa em um texto. Esses léxicos consistem em listas de palavras, cada uma associada a uma pontuação ou etiqueta que indica se a palavra tem uma conotação positiva, negativa ou neutra. Alguns léxicos também incluem intensidades para refletir o grau de emoção. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NRC Word-Emotion Association Lexicon (EmoLex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English Word</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aback</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abacus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abatement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abba</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abbot</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English Word  negative  positive\n",
       "0        aback         0         0\n",
       "1       abacus         0         0\n",
       "2      abandon         1         0\n",
       "3    abandoned         1         0\n",
       "4  abandonment         1         0\n",
       "5        abate         0         0\n",
       "6    abatement         0         0\n",
       "7         abba         0         1\n",
       "8        abbot         0         0\n",
       "9   abbreviate         0         0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista de palavras inglesas e o seu respetivo sentimento (positivo ou negativo)\n",
    "\n",
    "emolex = pd.read_csv(\n",
    "    \"NCR-lexicon.txt\",\n",
    "    skiprows=0,\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "\n",
    "emolex = emolex[[\"English Word\", \"negative\", \"positive\"]]\n",
    "\n",
    "emolex_dict = {row[\"English Word\"]: (row[\"positive\"], row[\"negative\"]) for index, row in emolex.iterrows()}\n",
    "\n",
    "emolex.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emolex_dict = {row[\"English Word\"]: (row[\"positive\"], row[\"negative\"]) for index, row in emolex.iterrows()}\n",
    "\n",
    "emolex_dict['happy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Afinn Lexicon ()\n",
    "\n",
    "Este léxico atribui a cada palavra uma pontuação de -5 a 5, indicando a intensidade do sentimento negativo ou positivo. É útil em contextos onde a intensidade precisa ser medida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandons</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abducted</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abduction</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abductions</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abhor</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abhorred</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abhorrent</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abhors</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  value\n",
       "0     abandon     -2\n",
       "1   abandoned     -2\n",
       "2    abandons     -2\n",
       "3    abducted     -2\n",
       "4   abduction     -2\n",
       "5  abductions     -2\n",
       "6       abhor     -3\n",
       "7    abhorred     -3\n",
       "8   abhorrent     -3\n",
       "9      abhors     -3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afinn = pd.read_csv(\"Afinn.csv\", encoding=\"latin1\")\n",
    "\n",
    "afinn.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afinn_dict = {row[\"word\"]: row[\"value\"] for index, row in afinn.iterrows()}\n",
    "\n",
    "afinn_dict['happy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bing Lexicon ()\n",
    "\n",
    "Este léxico é frequentemente usado em análise de sentimentos para identificar e contar o número de palavras positivas e negativas em um texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-faces</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abnormal</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abolish</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abominable</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abominably</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abominate</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abomination</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abort</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aborted</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aborts</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word sentiment\n",
       "0      2-faces  negative\n",
       "1     abnormal  negative\n",
       "2      abolish  negative\n",
       "3   abominable  negative\n",
       "4   abominably  negative\n",
       "5    abominate  negative\n",
       "6  abomination  negative\n",
       "7        abort  negative\n",
       "8      aborted  negative\n",
       "9       aborts  negative"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bing = pd.read_csv(\"Bing.csv\")\n",
    "\n",
    "bing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adiciona colunas binárias para os sentimentos\n",
    "bing['negative'] = (bing['sentiment'] == 'negative').astype(int)\n",
    "bing['positive'] = (bing['sentiment'] == 'positive').astype(int)\n",
    "\n",
    "# Remove a coluna de sentimento\n",
    "bing = bing.drop('sentiment', axis=1)\n",
    "\n",
    "#bing.head(10)\n",
    "\n",
    "bing_dict = {row[\"word\"]: (row[\"positive\"], row[\"negative\"]) for index, row in bing.iterrows()}\n",
    "\n",
    "bing_dict['happy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5ff90_row0_col1 {\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5ff90_row1_col1 {\n",
       "  background-color: #084a91;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5ff90_row2_col1 {\n",
       "  background-color: #4594c7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5ff90_row3_col1 {\n",
       "  background-color: #539ecd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5ff90_row4_col1 {\n",
       "  background-color: #72b2d8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5ff90_row5_col1 {\n",
       "  background-color: #85bcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ff90_row6_col1 {\n",
       "  background-color: #b8d5ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ff90_row7_col1 {\n",
       "  background-color: #c4daee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ff90_row8_col1 {\n",
       "  background-color: #cadef0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ff90_row9_col1 {\n",
       "  background-color: #d6e5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ff90_row10_col1 {\n",
       "  background-color: #dce9f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ff90_row11_col1 {\n",
       "  background-color: #ddeaf7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ff90_row12_col1 {\n",
       "  background-color: #e7f1fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ff90_row13_col1 {\n",
       "  background-color: #e8f1fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ff90_row14_col1 {\n",
       "  background-color: #f0f6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ff90_row15_col1 {\n",
       "  background-color: #f3f8fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ff90_row16_col1 {\n",
       "  background-color: #f4f9fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ff90_row17_col1, #T_5ff90_row18_col1 {\n",
       "  background-color: #f5fafe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5ff90_row19_col1 {\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5ff90\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5ff90_level0_col0\" class=\"col_heading level0 col0\" >Common_words</th>\n",
       "      <th id=\"T_5ff90_level0_col1\" class=\"col_heading level0 col1\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5ff90_row0_col0\" class=\"data row0 col0\" >the</td>\n",
       "      <td id=\"T_5ff90_row0_col1\" class=\"data row0 col1\" >154572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5ff90_row1_col0\" class=\"data row1 col0\" >i</td>\n",
       "      <td id=\"T_5ff90_row1_col1\" class=\"data row1 col1\" >142088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5ff90_row2_col0\" class=\"data row2 col0\" >and</td>\n",
       "      <td id=\"T_5ff90_row2_col1\" class=\"data row2 col1\" >105446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_5ff90_row3_col0\" class=\"data row3 col0\" >a</td>\n",
       "      <td id=\"T_5ff90_row3_col1\" class=\"data row3 col1\" >100114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_5ff90_row4_col0\" class=\"data row4 col0\" >it</td>\n",
       "      <td id=\"T_5ff90_row4_col1\" class=\"data row4 col1\" >88910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_5ff90_row5_col0\" class=\"data row5 col0\" >to</td>\n",
       "      <td id=\"T_5ff90_row5_col1\" class=\"data row5 col1\" >82734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_5ff90_row6_col0\" class=\"data row6 col0\" >of</td>\n",
       "      <td id=\"T_5ff90_row6_col1\" class=\"data row6 col1\" >65062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_5ff90_row7_col0\" class=\"data row7 col0\" >is</td>\n",
       "      <td id=\"T_5ff90_row7_col1\" class=\"data row7 col1\" >59759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_5ff90_row8_col0\" class=\"data row8 col0\" >this</td>\n",
       "      <td id=\"T_5ff90_row8_col1\" class=\"data row8 col1\" >56492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_5ff90_row9_col0\" class=\"data row9 col0\" >br</td>\n",
       "      <td id=\"T_5ff90_row9_col1\" class=\"data row9 col1\" >48939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_5ff90_row10_col0\" class=\"data row10 col0\" >for</td>\n",
       "      <td id=\"T_5ff90_row10_col1\" class=\"data row10 col1\" >44745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_5ff90_row11_col0\" class=\"data row11 col0\" >in</td>\n",
       "      <td id=\"T_5ff90_row11_col1\" class=\"data row11 col1\" >43813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_5ff90_row12_col0\" class=\"data row12 col0\" >my</td>\n",
       "      <td id=\"T_5ff90_row12_col1\" class=\"data row12 col1\" >37507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_5ff90_row13_col0\" class=\"data row13 col0\" >that</td>\n",
       "      <td id=\"T_5ff90_row13_col1\" class=\"data row13 col1\" >37000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_5ff90_row14_col0\" class=\"data row14 col0\" >but</td>\n",
       "      <td id=\"T_5ff90_row14_col1\" class=\"data row14 col1\" >32100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_5ff90_row15_col0\" class=\"data row15 col0\" >you</td>\n",
       "      <td id=\"T_5ff90_row15_col1\" class=\"data row15 col1\" >29969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_5ff90_row16_col0\" class=\"data row16 col0\" >not</td>\n",
       "      <td id=\"T_5ff90_row16_col1\" class=\"data row16 col1\" >29436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_5ff90_row17_col0\" class=\"data row17 col0\" >with</td>\n",
       "      <td id=\"T_5ff90_row17_col1\" class=\"data row17 col1\" >28772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_5ff90_row18_col0\" class=\"data row18 col0\" >have</td>\n",
       "      <td id=\"T_5ff90_row18_col1\" class=\"data row18 col1\" >28346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5ff90_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_5ff90_row19_col0\" class=\"data row19 col0\" >was</td>\n",
       "      <td id=\"T_5ff90_row19_col1\" class=\"data row19 col1\" >27304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x320dc4e10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Top 20 palavras mais comuns\n",
    "words = re.findall(r'\\w+', ' '.join(train_data['review'].str.lower()))\n",
    "top = Counter(words)\n",
    "temp = pd.DataFrame(top.most_common(20))\n",
    "temp.columns = ['Common_words', 'count']\n",
    "temp.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Limpeza do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try and except: foi utilizado em quase todas as funções porque mais para a frente vamos fazer as combinações e por isso não vamos precisar sempre de tokenizar pois vai bastar uma vez, então se der erro ele apenas passa o text para o words. O words essencialmente nesses casos vai ser já a lista de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Função de limpeza de texto\n",
    "def clean(text):\n",
    "    \n",
    "    # Transformar em minúsculas\n",
    "    text = text.lower()\n",
    "    # Remover código HTML\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "    # Remover URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https?\\:\\/\\/\\S+\", \"\", text)\n",
    "    # Remover menções a usuários (não comum em reviews da Amazon)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    # Remover hashtags (também não comum em reviews da Amazon)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    # Remover entidades HTML (&amp;, &lt;, etc.)\n",
    "    text = re.sub(r\"&\\w+;\", \"\", text)\n",
    "    # Remover números (avaliações numéricas, preços, etc.)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # Substituir caracteres de pontuação por espaços\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    # Remover espaços múltiplos e linhas novas\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    #Remover espaços no início e no fim\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# train_clean = train_data[\"review\"].apply(clean)\n",
    "# test_clean = test_data[\"review\"].apply(clean)\n",
    "\n",
    "\n",
    "# train_clean.to_csv(\n",
    "#    \"/Users/marianaborralho/Desktop/M. Ciencia de Dados/2 Semestre/Text Mining/Trabalho/train_clean.csv\",\n",
    "#    index=False,\n",
    "# )\n",
    "# test_clean.to_csv(\n",
    "#    \"/Users/marianaborralho/Desktop/M. Ciencia de Dados/2 Semestre/Text Mining/Trabalho/train_clean.csv\",\n",
    "#    index=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi, I have to have black tea everyday. I have done it in the past 25 years and it's a habit of mine. Ahmad tea is one of my favorite brands and I highly recommend it if you like black tea. I think this is going to remain my favorite for the next 100 years or so :)\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"review\"][157]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_clean[157]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marianaborralho/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "# Função de tokenização\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "# train_tok = train_data[\"review\"].apply(tokenize)\n",
    "\n",
    "# test_tok = test_data[\"review\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tok[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Função que remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    \n",
    "    try:\n",
    "        words = word_tokenize(text)\n",
    "    except:\n",
    "        words = text\n",
    "        \n",
    "    words = [word for word in words if word not in nltk_stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "# train_stopwords = train_data[\"review\"].apply(remove_stopwords)\n",
    "# test_stopwords = test_data[\"review\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ordered this for my wife as it was reccomended by our daughter.  She has this almost every morning and likes all flavors.  She\\'s happy, I\\'m happy!!!<br /><a href=\"http://www.amazon.com/gp/product/B001EO5QW8\">McCANN\\'S Instant Irish Oatmeal, Variety Pack of Regular, Apples & Cinnamon, and Maple & Brown Sugar, 10-Count Boxes (Pack of 6)</a>'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"review\"][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stopwords[28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Função de stemming\n",
    "def stem(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    try:\n",
    "        words = word_tokenize(text)\n",
    "    except:\n",
    "        words = text\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "# train_stem = train_data['review'].apply(stem)\n",
    "# test_stem = test_data['review'].apply(stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ordered this for my wife as it was reccomended by our daughter.  She has this almost every morning and likes all flavors.  She\\'s happy, I\\'m happy!!!<br /><a href=\"http://www.amazon.com/gp/product/B001EO5QW8\">McCANN\\'S Instant Irish Oatmeal, Variety Pack of Regular, Apples & Cinnamon, and Maple & Brown Sugar, 10-Count Boxes (Pack of 6)</a>'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"review\"][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stem[28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Função de lemmatization\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    try:\n",
    "        words = word_tokenize(text)\n",
    "    except:\n",
    "        words = text\n",
    "        \n",
    "    words = [lemmatizer.lemmatize(word, \"v\") for word in words]\n",
    "    return words\n",
    "\n",
    "# train_lemmatized = train_data[\"review\"].apply(lemmatize)\n",
    "# test_lemmatized = test_data[\"review\"].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ordered this for my wife as it was reccomended by our daughter.  She has this almost every morning and likes all flavors.  She\\'s happy, I\\'m happy!!!<br /><a href=\"http://www.amazon.com/gp/product/B001EO5QW8\">McCANN\\'S Instant Irish Oatmeal, Variety Pack of Regular, Apples & Cinnamon, and Maple & Brown Sugar, 10-Count Boxes (Pack of 6)</a>'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"review\"][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_lemmatized[28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Contrações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "# Função que trata das contrações\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "# train_expanded = train_data[\"review\"].apply(expand_contractions)\n",
    "# test_expanded = test_data[\"review\"].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ordered this for my wife as it was reccomended by our daughter.  She has this almost every morning and likes all flavors.  She\\'s happy, I\\'m happy!!!<br /><a href=\"http://www.amazon.com/gp/product/B001EO5QW8\">McCANN\\'S Instant Irish Oatmeal, Variety Pack of Regular, Apples & Cinnamon, and Maple & Brown Sugar, 10-Count Boxes (Pack of 6)</a>'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"review\"][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_expanded[28]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- POS tagging (Part-of-Speech tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foram removidas 3 classes: Preposições, Conjunções e Pronomes\n",
    "\n",
    "Nota: Algumas destas classes podem ser removidas usando a função de stopwords (são muito comuns e geralmente não contribuem muito para o significado de uma frase)\n",
    "\n",
    "Ver se faz sentido adicionar/retirar alguma classe\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        words = word_tokenize(text)\n",
    "    except:\n",
    "        words = text\n",
    "    \n",
    "    # Aplica o POS tagging a cada palavra\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    # Filtra palavras que são preposições (IN), conjunções (CC) ou pronomes (PRP, PRP$)\n",
    "    filtered_words = [word for word, tag in pos_tags if tag not in ['IN', 'CC', 'PRP', 'PRP$']]\n",
    "\n",
    "    # Junta as palavras filtradas de volta em uma string\n",
    "    return filtered_words\n",
    "\n",
    "# Exemplo \n",
    "#text_example = \"The quick brown fox jumps over the lazy dog\"\n",
    "#filtered_text = pos_tagging(text_example)\n",
    "#print(filtered_text)\n",
    "\n",
    "# train_pos = train_data[\"review\"].apply(pos_tagging)\n",
    "# test_pos = test_data[\"review\"].apply(pos_tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The candy is just red , No flavor . Just  plan and chewy .  I would never buy them again'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['review'][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pos[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tratamento da Negação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper: Effect of Negation in Sentences on Sentiment Analysis and Polarity\n",
    "Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "# Função que trata da negação\n",
    "def negation(tokens):\n",
    "    try:\n",
    "        tokens = word_tokenize(tokens)\n",
    "    except:\n",
    "        tokens = tokens\n",
    "    return mark_negation(tokens)\n",
    "\n",
    "# train_negation = train_data[\"review\"].apply(negation)\n",
    "# test_negation = test_data[\"review\"].apply(negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The candy is just red , No flavor . Just  plan and chewy .  I would never buy them again'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['review'][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_negation[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função **mark_negation** da biblioteca NLTK anexa um sufixo '_NEG' a todas as palavras que aparecem após uma palavra de negação até a próxima pontuação. As palavras de negação padrão que a função considera são 'not', 'no', 'never', 'nobody', 'none', 'nowhere', 'nothing', 'neither', 'nor', 'n', 'nt', 'n’t'. A pontuação padrão que a função considera para terminar o uso de negação é '.', ':', ';', '!', '?'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problema**: Não considera a virgula como pontuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "\n",
    "# Função Função que identifica e marca negações sintáticas\n",
    "def mark_syntactic_negation(text):\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    negation_words = ['not', 'no', 'never', 'nobody', 'none', 'nowhere', 'nothing', 'neither', 'nor', 'n', 'nt']\n",
    "    punctuation = ['.', ':', ';', '!', '?', ',']  \n",
    "    suffixed = []\n",
    "    neg_scope = False\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in negation_words:\n",
    "            neg_scope = True\n",
    "            suffixed.append(token)\n",
    "            continue\n",
    "\n",
    "        if token in punctuation:\n",
    "            neg_scope = False\n",
    "\n",
    "        if neg_scope:\n",
    "            token = \"NOT_\" + token \n",
    "\n",
    "        suffixed.append(token)\n",
    "\n",
    "    return suffixed\n",
    "\n",
    "train_syntactic_negation = train_data[\"review\"].apply(mark_syntactic_negation)\n",
    "test_syntactic_negation_ = test_data[\"review\"].apply(mark_syntactic_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"review\"][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'do',\n",
       " 'not',\n",
       " 'NOT_know',\n",
       " 'NOT_if',\n",
       " 'NOT_it',\n",
       " 'NOT_is',\n",
       " 'NOT_the',\n",
       " 'NOT_cactus',\n",
       " 'NOT_or',\n",
       " 'NOT_the',\n",
       " 'NOT_tequila',\n",
       " 'NOT_or',\n",
       " 'NOT_just',\n",
       " 'NOT_the',\n",
       " 'NOT_unique',\n",
       " 'NOT_combination',\n",
       " 'NOT_of',\n",
       " 'NOT_ingredients',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'flavour',\n",
       " 'of',\n",
       " 'this',\n",
       " 'hot',\n",
       " 'sauce',\n",
       " 'makes',\n",
       " 'it',\n",
       " 'one',\n",
       " 'of',\n",
       " 'a',\n",
       " 'kind',\n",
       " '!',\n",
       " 'we',\n",
       " 'picked',\n",
       " 'up',\n",
       " 'a',\n",
       " 'bottle',\n",
       " 'once',\n",
       " 'on',\n",
       " 'a',\n",
       " 'trip',\n",
       " 'we',\n",
       " 'were',\n",
       " 'on',\n",
       " 'and',\n",
       " 'brought',\n",
       " 'it',\n",
       " 'back',\n",
       " 'home',\n",
       " 'with',\n",
       " 'us',\n",
       " 'and',\n",
       " 'were',\n",
       " 'totally',\n",
       " 'blown',\n",
       " 'away',\n",
       " '!',\n",
       " 'when',\n",
       " 'we',\n",
       " 'realized',\n",
       " 'that',\n",
       " 'we',\n",
       " 'simply',\n",
       " 'could',\n",
       " 'not',\n",
       " 'NOT_find',\n",
       " 'NOT_it',\n",
       " 'NOT_anywhere',\n",
       " 'NOT_in',\n",
       " 'NOT_our',\n",
       " 'NOT_city',\n",
       " 'NOT_we',\n",
       " 'NOT_were',\n",
       " 'NOT_bummed.',\n",
       " 'NOT_<',\n",
       " 'NOT_br',\n",
       " 'NOT_/',\n",
       " 'NOT_>',\n",
       " 'NOT_<',\n",
       " 'NOT_br',\n",
       " 'NOT_/',\n",
       " 'NOT_>',\n",
       " 'NOT_now',\n",
       " ',',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'magic',\n",
       " 'of',\n",
       " 'the',\n",
       " 'internet',\n",
       " ',',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " 'case',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sauce',\n",
       " 'and',\n",
       " 'are',\n",
       " 'ecstatic',\n",
       " 'because',\n",
       " 'of',\n",
       " 'it.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'if',\n",
       " 'you',\n",
       " 'love',\n",
       " 'hot',\n",
       " 'sauce',\n",
       " '..',\n",
       " 'i',\n",
       " 'mean',\n",
       " 'really',\n",
       " 'love',\n",
       " 'hot',\n",
       " 'sauce',\n",
       " ',',\n",
       " 'but',\n",
       " 'do',\n",
       " 'not',\n",
       " 'NOT_want',\n",
       " 'NOT_a',\n",
       " 'NOT_sauce',\n",
       " 'NOT_that',\n",
       " 'NOT_tastelessly',\n",
       " 'NOT_burns',\n",
       " 'NOT_your',\n",
       " 'NOT_throat',\n",
       " ',',\n",
       " 'grab',\n",
       " 'a',\n",
       " 'bottle',\n",
       " 'of',\n",
       " 'tequila',\n",
       " 'picante',\n",
       " 'gourmet',\n",
       " 'de',\n",
       " 'inclan',\n",
       " '.',\n",
       " 'just',\n",
       " 'realize',\n",
       " 'that',\n",
       " 'once',\n",
       " 'you',\n",
       " 'taste',\n",
       " 'it',\n",
       " ',',\n",
       " 'you',\n",
       " 'will',\n",
       " 'never',\n",
       " 'NOT_want',\n",
       " 'NOT_to',\n",
       " 'NOT_use',\n",
       " 'NOT_any',\n",
       " 'NOT_other',\n",
       " 'NOT_sauce.',\n",
       " 'NOT_<',\n",
       " 'NOT_br',\n",
       " 'NOT_/',\n",
       " 'NOT_>',\n",
       " 'NOT_<',\n",
       " 'NOT_br',\n",
       " 'NOT_/',\n",
       " 'NOT_>',\n",
       " 'NOT_thank',\n",
       " 'NOT_you',\n",
       " 'NOT_for',\n",
       " 'NOT_the',\n",
       " 'NOT_personal',\n",
       " ',',\n",
       " 'incredible',\n",
       " 'service',\n",
       " '!']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_syntactic_negation[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'NOT_uncommon', 'for', 'NOT_disorganized', 'plans', 'to', 'succeed', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Função que identifica e marca negações morfológicas\n",
    "def mark_morphological_negation(text):\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    negation_prefixes = ['ab','il','ir','un', 'in', 'im', 'dis', 'non']\n",
    "    suffixed = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if any(token.startswith(prefix) for prefix in negation_prefixes):\n",
    "            # Marcar o token com um sufixo especial para indicar a negação\n",
    "            token = \"NOT_\" + token\n",
    "        suffixed.append(token)\n",
    "\n",
    "    return suffixed\n",
    "\n",
    "# Exemplo\n",
    "example_text = \"It is uncommon for disorganized plans to succeed.\"\n",
    "processed_text = mark_morphological_negation(example_text)\n",
    "print(processed_text)\n",
    "\n",
    "\n",
    "train_morphological_negation = train_data[\"review\"].apply(mark_morphological_negation)\n",
    "test_morphological_negation_ = test_data[\"review\"].apply(mark_morphological_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['product',\n",
       " 'arrived',\n",
       " 'labeled',\n",
       " 'as',\n",
       " 'jumbo',\n",
       " 'salted',\n",
       " 'peanuts',\n",
       " '...',\n",
       " 'the',\n",
       " 'peanuts',\n",
       " 'were',\n",
       " 'actually',\n",
       " 'small',\n",
       " 'sized',\n",
       " 'NOT_unsalted',\n",
       " '.',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'if',\n",
       " 'this',\n",
       " 'was',\n",
       " 'an',\n",
       " 'error',\n",
       " 'or',\n",
       " 'if',\n",
       " 'the',\n",
       " 'vendor',\n",
       " 'NOT_intended',\n",
       " 'to',\n",
       " 'represent',\n",
       " 'the',\n",
       " 'product',\n",
       " 'as',\n",
       " '``',\n",
       " 'jumbo',\n",
       " \"''\",\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_morphological_negation[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classificação final a atribuir ao texto\n",
    "\n",
    "**ideia geral:** quantificar quantas palavras do texto a analisar estão classificadas como Positive e Negative e, consoante a classe mais frequente, decidir qual a classificação final a atribuír ao texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentiment_words(tokens, lexicon):\n",
    "    # Initialize counters\n",
    "    counts = {\"positive\": 0, \"negative\": 0}\n",
    "    not_found_words = set()  # Use set for O(1) lookups\n",
    "\n",
    "    last_sentiment = None  # Tracks the last sentiment found\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in lexicon:\n",
    "            positive_score, negative_score = lexicon[token]\n",
    "            counts[\"positive\"] += positive_score\n",
    "            counts[\"negative\"] += negative_score\n",
    "            \n",
    "            # Update the last sentiment observed\n",
    "            if positive_score > negative_score:\n",
    "                last_sentiment = \"positive\"\n",
    "            elif negative_score > positive_score:\n",
    "                last_sentiment = \"negative\"\n",
    "        elif '_NEG' in token:\n",
    "            counts[\"negative\"] += 3\n",
    "            last_sentiment = \"negative\"\n",
    "        else:\n",
    "            not_found_words.add(token)  # Add the word to the set\n",
    "    \n",
    "    not_found_percentage = (len(not_found_words) / len(tokens)) * 100\n",
    "    \n",
    "    # Determine the overall sentiment\n",
    "    if counts[\"positive\"] > counts[\"negative\"]:\n",
    "        sentiment = \"positive\"\n",
    "    elif counts[\"negative\"] > counts[\"positive\"]:\n",
    "        sentiment = \"negative\"\n",
    "    else:\n",
    "        # Strategy for ties: use the sentiment of the last word with sentiment\n",
    "        sentiment = last_sentiment if last_sentiment else random.choice([\"positive\", \"negative\"])\n",
    "    \n",
    "    return sentiment, list(not_found_words), not_found_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentiment_words_Afinn(tokens, lexicon):\n",
    "    \n",
    "    counts = 0\n",
    "    counter_not_found = 0\n",
    "    not_found_words = []  # Lista para armazenar palavras não encontradas no léxico\n",
    "    last_sentiment = None  # Acompanha o último sentimento encontrado\n",
    "    \n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in lexicon:\n",
    "            score = lexicon[token]\n",
    "            counts += score\n",
    "            \n",
    "            # Atualiza o último sentimento observado\n",
    "            if score > 0:\n",
    "                last_sentiment = \"positive\"\n",
    "            elif score < 0:\n",
    "                last_sentiment = \"negative\"\n",
    "        elif '_NEG' in token:\n",
    "            counts -= 5\n",
    "            last_sentiment = \"negative\"    \n",
    "        else:\n",
    "            counter_not_found += 1\n",
    "            not_found_words.append(token)  # Adiciona a palavra à lista de não encontradas\n",
    "            \n",
    "    total_tokens = len(tokens)\n",
    "    not_found_percentage = (counter_not_found / total_tokens) * 100\n",
    "    \n",
    "    # Determina o sentimento geral\n",
    "    if counts > 0:\n",
    "        sentiment = \"positive\"\n",
    "    elif counts < 0:\n",
    "        sentiment = \"negative\"\n",
    "    else:\n",
    "        # Estratégia de desempate: usa o último sentimento observado\n",
    "        if last_sentiment:\n",
    "            sentiment = last_sentiment\n",
    "        else:\n",
    "            # Se não houve palavras com sentimento detectado, escolhe aleatoriamente\n",
    "            sentiment = random.choice([\"positive\", \"negative\"])  # Ou outra estratégia padrão\n",
    "    \n",
    "    return sentiment, not_found_words, not_found_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(data, actual_col, predicted_col, not_found_col):\n",
    "\n",
    "    correct_predictions = data[data[actual_col] == data[predicted_col]]\n",
    "    accuracy = len(correct_predictions) / len(data) * 100\n",
    "    not_found_average = data[not_found_col].mean()  # Calcula a média de percentagens de palavras não encontradas\n",
    "    return accuracy, not_found_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideia geral: queremos testar todas as combinações quer seja com os diferentes lexicos como os metodos de processamento. Clean e Expand Contractions serão sempre os primeiros metodos a ser aplicados pois não é preciso ser tokenizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Diferentes combinacoes de pre process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations: 100%|██████████| 333/333 [31:14<00:00,  5.63s/combo]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm # Barra de progresso\n",
    "\n",
    "# Lista de métodos de processamento de texto\n",
    "methods = [clean, expand_contractions, negation, pos_tagging,remove_stopwords, lemmatize, stem]\n",
    "# Dicionário de léxicos\n",
    "lexicon_names = {'afinn_dict': afinn_dict, 'bing_dict': bing_dict, 'emolex_dict': emolex_dict}\n",
    "\n",
    "final_combinations = []\n",
    "\n",
    "# Combina métodos individualmete\n",
    "for method in methods:\n",
    "    final_combinations.append((method,))\n",
    "\n",
    "# Combina 2 métodos\n",
    "for (method1, method2) in itertools.combinations(methods, 2):\n",
    "    final_combinations.append((method1, method2))\n",
    "\n",
    "# Combina 3 métodos\n",
    "for combination in itertools.combinations(methods, 3):\n",
    "    final_combinations.append(combination)\n",
    "\n",
    "# Combina 4 métodos\n",
    "for combination in itertools.combinations(methods, 4):\n",
    "    final_combinations.append(combination)\n",
    "\n",
    "# Combina todos os métodos\n",
    "for combination in itertools.combinations(methods, 5):\n",
    "    final_combinations.append(combination)\n",
    "\n",
    "\n",
    "# Verifica se os métodos 'clean' e 'expand_contractions'estão na ordem correta quando aplicável e evita que incluam combinações com \n",
    "# lemmatize e stem em simultâneo\n",
    "new_combinations = []\n",
    "for combination in final_combinations:\n",
    "    if clean in combination and expand_contractions in combination or clean in combination or expand_contractions in combination:\n",
    "        if clean in combination and combination.index(clean) == 0:\n",
    "            new_combinations.append(combination)\n",
    "        elif expand_contractions in combination and combination.index(expand_contractions) == 0:\n",
    "            new_combinations.append(combination)\n",
    "        elif clean in combination and expand_contractions in combination and combination.index(clean) == 0 and combination.index(expand_contractions) == 1:\n",
    "            new_combinations.append(combination)\n",
    "        else:\n",
    "            continue\n",
    "    if lemmatize in combination and stem in combination:\n",
    "        continue\n",
    "    else:\n",
    "        new_combinations.append(combination)\n",
    "\n",
    "final_combinations = new_combinations\n",
    "#check for duplicates\n",
    "#print(len(final_combinations))\n",
    "final_combinations = list(set(final_combinations))\n",
    "#print(len(final_combinations))\n",
    "\n",
    "lexicons = list(lexicon_names.keys())\n",
    "final_combinations = list(itertools.product(final_combinations, lexicons))\n",
    "\n",
    "results = []\n",
    "\n",
    "for combo in tqdm(final_combinations, desc='Processing combinations', unit='combo'):\n",
    "\n",
    "    combo_data = test_data.copy() # Cópia dos dados de teste\n",
    "    # Aplica métodos de processamento de texto\n",
    "    text_processing_methods = combo[0]\n",
    "\n",
    "    for method in text_processing_methods:\n",
    "        combo_data['review'] = combo_data['review'].apply(method)\n",
    "\n",
    "    # Aplica o léxico escolhido\n",
    "    lexicon_name = combo[1]\n",
    "    lexicon = lexicon_names[lexicon_name]\n",
    "    \n",
    "    if lexicon == afinn_dict:\n",
    "        # Função específica para AFINN\n",
    "        if text_processing_methods == (clean, expand_contractions) or text_processing_methods == (clean,) or text_processing_methods == (expand_contractions,):\n",
    "            combo_data[['sentiment_predicted', 'missing_words', 'not_found_percentage']] = combo_data['review'].apply(\n",
    "                lambda review: count_sentiment_words_Afinn(word_tokenize(review), lexicon)\n",
    "            ).apply(pd.Series)\n",
    "        else:\n",
    "            combo_data[['sentiment_predicted', 'missing_words', 'not_found_percentage']] = combo_data['review'].apply(\n",
    "                lambda review: count_sentiment_words_Afinn(review, lexicon)\n",
    "            ).apply(pd.Series)\n",
    "    else:\n",
    "        # Função geral para outros léxicos\n",
    "        if text_processing_methods == (clean, expand_contractions) or text_processing_methods == (clean,) or text_processing_methods == (expand_contractions,):\n",
    "            combo_data[['sentiment_predicted', 'missing_words', 'not_found_percentage']] = combo_data['review'].apply(\n",
    "                lambda review: count_sentiment_words(word_tokenize(review), lexicon)\n",
    "            ).apply(pd.Series)\n",
    "    \n",
    "        else:\n",
    "            combo_data[['sentiment_predicted', 'missing_words', 'not_found_percentage']] = combo_data['review'].apply(\n",
    "                lambda review: count_sentiment_words(review, lexicon)\n",
    "            ).apply(pd.Series)\n",
    "\n",
    "        \n",
    "    accuracy, not_found_average = calculate_accuracy(combo_data, 'sentiment', 'sentiment_predicted', 'not_found_percentage')\n",
    "    \n",
    "    report_dict = classification_report(combo_data['sentiment'], combo_data['sentiment_predicted'], output_dict=True)\n",
    "    \n",
    "    # Processa métricas do relatório de classificação\n",
    "    report_metrics = {\n",
    "        f\"{label}_{metric}\": report_dict[label][metric]\n",
    "        for label in report_dict if label not in ['accuracy', 'macro avg', 'weighted avg']\n",
    "        for metric in ['precision', 'recall', 'f1-score', 'support']\n",
    "    }\n",
    "\n",
    "     # Combina métricas do relatório de classificação com outros resultados e adicionar à lista de resultados\n",
    "    results.append({\n",
    "        'combination': (\", \".join([method.__name__ for method in text_processing_methods]), lexicon_name),\n",
    "        'accuracy': accuracy,\n",
    "        'not_found_average': not_found_average,\n",
    "        **report_metrics  # Desempacota as métricas do relatório de classificação\n",
    "    })\n",
    "    \n",
    "# Converte resultados em DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Guarda os resultados em um arquivo CSV\n",
    "results_df.to_csv(\"text_processing_combinations_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se quisermos executar algum método individualmente temos esta forma   \n",
    "\n",
    "test_data[['sentiment_predicted_tokenized_cleaned_expand_remove_stem', 'missing_words', 'not_found_percentage']] = test_data[\"review\"].apply(lambda review: count_sentiment_words(stem(pos_tagging(expand_contractions(clean(review)))), emolex_dict)).apply(pd.Series)\n",
    "accuracy, not_found_average = calculate_accuracy(test_data, 'sentiment', 'sentiment_predicted_tokenized_cleaned_expand_remove_stem', 'not_found_percentage')\n",
    "print(f\"Average percentage of not found words: {not_found_average}%\")\n",
    "\n",
    "\n",
    "print(classification_report(test_data[\"sentiment\"], test_data[\"sentiment_predicted_tokenized_cleaned_expand_remove_stem\"] ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se quisermos executar algum método individualmente temos esta forma   \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "test_data[['sentiment_predicted_tokenized_cleaned', 'missing_words', 'not_found_percentage']] = test_data[\"review\"].apply(lambda review: count_sentiment_words_Afinn(word_tokenize(clean(review)), afinn_dict)).apply(pd.Series)\n",
    "accuracy, not_found_average = calculate_accuracy(test_data, 'sentiment', 'sentiment_predicted_tokenized_cleaned', 'not_found_percentage')\n",
    "print(f\"Average percentage of not found words: {not_found_average}%\")\n",
    "\n",
    "\n",
    "print(classification_report(test_data[\"sentiment\"], test_data[\"sentiment_predicted_tokenized_cleaned\"] ))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "test_data[['sentiment_predicted_tokenized_cleaned', 'missing_words', 'not_found_percentage']] = test_data[\"review\"].apply(lambda review: count_sentiment_words(word_tokenize(clean(review)), bing_dict)).apply(pd.Series)\n",
    "accuracy, not_found_average = calculate_accuracy(test_data, 'sentiment', 'sentiment_predicted_tokenized_cleaned', 'not_found_percentage')\n",
    "print(f\"Average percentage of not found words: {not_found_average}%\")\n",
    "\n",
    "\n",
    "print(classification_report(test_data[\"sentiment\"], test_data[\"sentiment_predicted_tokenized_cleaned\"] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que falta para ficar finalizado ate ao ponto 1.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Léxicos: procurar alguma bibliografia que reporte a utilização de léxicos de polaridade por forma a decidir qual a melhor abordagem\n",
    "- Fazer alguma coisa em relacao aos tokens que nao sao encontrados dentro do dicionario \n",
    "- Experimentar as diferentes combinacoes de pre process text (stemming, lemmatization, stopwords, etc) \n",
    "(Juntar resultados num dataframe e exportar para csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Treino de um modelo (aprendizagem automática)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ª Abordagem mais simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:547: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8604987350921576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86      2782\n",
      "           1       0.87      0.84      0.86      2752\n",
      "\n",
      "    accuracy                           0.86      5534\n",
      "   macro avg       0.86      0.86      0.86      5534\n",
      "weighted avg       0.86      0.86      0.86      5534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', analyzer=stem)\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(train_data_balanced['review'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(train_data_balanced['sentiment'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Treino de um modelo (aprendizagem automática)\n",
    "\n",
    "Nesta seção, abordaremos o processo de treinar um modelo de aprendizagem automática com o objetivo de melhorar o desempenho da classificação de dados. Seguiremos os seguintes passos:\n",
    "\n",
    "## 4.1 Treinamento do Modelo\n",
    "\n",
    "1. Selecione uma ferramenta de classificação adequada, como **scikit-learn**, TensorFlow, PyTorch, etc.\n",
    "2. Prepare os dados de treino, garantindo que estão devidamente limpos e formatados para o processo de treinamento.\n",
    "3. Configure e treine o modelo com o conjunto de dados de treino.\n",
    "\n",
    "    - Várias configurações podem ser testadas:\n",
    "        - Número de features.\n",
    "        - Tratamento de maiúsculas e minúsculas.\n",
    "        - Diferentes métodos de pré-processamento de texto (normalização, stemming, lematização, etc.).\n",
    "        - Inclusão ou exclusão de informações de part-of-speech tagging.\n",
    "        - Utilização de entidades nomeadas reconhecidas no texto.\n",
    "        - Incorporação de embeddings pré-treinados, como GloVe.\n",
    "        - Se houver desbalanceamento de classes usar \n",
    "\n",
    "## 4.2 Avaliação do Modelo\n",
    "\n",
    "1. Aplique o modelo treinado ao conjunto de dados de teste.\n",
    "2. Avalie o desempenho do modelo utilizando métricas adequadas, como precisão, recall, F1-score, etc.\n",
    "3. Compare os resultados obtidos com os do modelo anterior para avaliar as melhorias.\n",
    "\n",
    "## 4.3 Documentação e Experimentação\n",
    "\n",
    "- Realize experimentos variados para entender o impacto de diferentes features e técnicas de pré-processamento.\n",
    "- Documente cada experimento, incluindo a configuração usada e os resultados obtidos.\n",
    "- Registre observações e conclusões relevantes a cada experimento.\n",
    "\n",
    "## 4.4 Referências\n",
    "\n",
    "- Alguns links que podem ajudar nesta tarefa \n",
    "    - https://www.kaggle.com/code/benroshan/sentiment-analysis-amazon-reviews#Extracting-Features-from-Cleaned-reviews\n",
    "    - https://github.com/jesseqzhen/NLP_Sentiment_Analysis\n",
    "    - https://www.kaggle.com/code/yacharki/binary-classification-amazon-reviews-84-lstm\n",
    "    - https://www.kaggle.com/code/mammadabbasli/amazon-reviews-analysis-logisticregression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Utilização de transformadores para classificação\n",
    "\n",
    "- https://medium.com/@minamehdinia213/fine-tunning-bert-model-for-amazon-product-review-and-deploying-it-into-hugging-face-model-hub-7d259839d556\n",
    "\n",
    "A aplicação de modelos pré-treinados baseados em transformadores representa uma técnica poderosa no campo da aprendizagem automática, especialmente para tarefas de classificação de texto. O processo será dividido em duas etapas principais, conforme descrito a seguir:\n",
    "\n",
    "## 5.1 Experimentação com Pipelines Pré-definidos\n",
    "\n",
    "1. Escolha um ou mais modelos baseados em transformadores disponíveis, como BERT, GPT, Transformer-XL, etc.\n",
    "2. Utilize pipelines pré-definidos oferecidos por bibliotecas como Hugging Face's Transformers para aplicar rapidamente o modelo aos seus dados.\n",
    "   \n",
    "    Exemplos de comandos de pipelines podem incluir:\n",
    "    \n",
    "    ```python\n",
    "    from transformers import pipeline\n",
    "    classifier = pipeline('text-classification', model='bert-base-uncased')\n",
    "    ```\n",
    "    \n",
    "3. Avalie o desempenho desses modelos em seu conjunto de dados sem qualquer ajuste adicional, usando métricas padrão de classificação.\n",
    "\n",
    "## 5.2 Fine-tuning de um Modelo Pré-treinado\n",
    "\n",
    "1. Selecione um modelo pré-treinado apropriado para o seu conjunto de dados e a tarefa de classificação em questão.\n",
    "2. Adapte o modelo ao seu conjunto de dados específico, o que é conhecido como fine-tuning. Isso envolve o treinamento do modelo em seu conjunto de dados, ajustando os pesos do modelo pré-treinado para melhor se adequar à sua tarefa específica.\n",
    "3. Durante o fine-tuning, experimente com diferentes hiperparâmetros, como taxa de aprendizado, número de épocas, tamanho do lote e outros relevantes para o modelo escolhido.\n",
    "   \n",
    "    Exemplo de código para fine-tuning:\n",
    "    \n",
    "    ```python\n",
    "    from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    training_args = TrainingArguments(output_dir='./results', num_train_epochs=3, ...)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    ```\n",
    "    \n",
    "4. Após o fine-tuning, avalie novamente o modelo em seu conjunto de dados, usando as mesmas métricas de classificação para comparar o desempenho com o modelo antes do ajuste."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
