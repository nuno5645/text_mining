{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nuno/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nuno/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/nuno/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nuno/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bibliotecas\n",
    "\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from spacy.tokens import Doc\n",
    "import stanza\n",
    "import ntk\n",
    "from nltk.corpus import stopwords  \n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import nltk\n",
    "from sklearn.utils import resample\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Análise Sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dados\n",
    "\n",
    "amazon_reviews: Conjunto não balanceado, com cerca de 50000 reviews de produtos da empresa Amazon, anotadas com as etiquetas “positive” e “negative”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             review\n",
       "0  negative  Product arrived labeled as Jumbo Salted Peanut...\n",
       "1  positive  This is a confection that has been around a fe...\n",
       "2  negative  If you are looking for the secret ingredient i...\n",
       "3  positive  Great taffy at a great price.  There was a wid...\n",
       "4  positive  This saltwater taffy had great flavors and was..."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar os dados de treino\n",
    "train_data = pd.read_csv(\"amazon_reviews_train.csv\")\n",
    "\n",
    "# Carregar os dados de teste\n",
    "test_data = pd.read_csv(\"amazon_reviews_test.csv\")\n",
    "\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values: 0\n",
      "NA values: False\n",
      "Null values: 0\n",
      "NA values: False\n"
     ]
    }
   ],
   "source": [
    "# Verificar os NA no conjunto de treino\n",
    "print(\"Null values:\",train_data.isnull().values.sum())\n",
    "print(\"NA values:\", train_data.isna().values.any())\n",
    "\n",
    "\n",
    "# Verificar os NA no conjunto de teste\n",
    "print(\"Null values:\",test_data.isnull().values.sum())\n",
    "print(\"NA values:\", test_data.isna().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    37835\n",
       "negative    11067\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifica o balanceamento das classes\n",
    "train_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    1676\n",
       "negative     741\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifica o balanceamento das classes\n",
    "test_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = sns.color_palette('deep')\n",
    "\n",
    "plt.figure(figsize=(6,4), tight_layout=True)\n",
    "plt.bar(x=['Positive', 'Negative'],\n",
    "        height=train_data['sentiment'].value_counts(),\n",
    "        color=colors[:2])\n",
    "plt.title('Sentiment in Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conjunto de treino não balanceado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "positive    37835\n",
      "negative    11067\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "negative    11067\n",
      "positive    11067\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Configura o undersampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Separa os dados e as etiquetas\n",
    "X_train = train_data[['review']]\n",
    "y_train = train_data['sentiment']\n",
    "\n",
    "# Aplica o undersampling\n",
    "X_res, y_res = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Verifica o novo balanceamento das classes\n",
    "#print(pd.Series(y_res).value_counts())\n",
    "\n",
    "# Criar um novo dataframe com os dados balanceados\n",
    "train_data_balanced = pd.concat([X_res, pd.Series(y_res, name='sentiment')], axis=1)\n",
    "\n",
    "print(train_data['sentiment'].value_counts())\n",
    "print(train_data_balanced['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Definição de um baseline usando ferramentas já existentes\n",
    "\n",
    "- TextBlob\n",
    "- Vader Sentiment\n",
    "- Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar biblioteca textblob\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar biblioteca vaderSentiment\n",
    "# !pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar biblioteca stanza\n",
    "# !pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment', 'review'], dtype='object')\n",
      "sentiment\n",
      "1    1676\n",
      "0     741\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.36      0.48       741\n",
      "           1       0.77      0.94      0.85      1676\n",
      "\n",
      "    accuracy                           0.76      2417\n",
      "   macro avg       0.75      0.65      0.67      2417\n",
      "weighted avg       0.76      0.76      0.74      2417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Verificar os nomes das colunas\n",
    "print(test_data.columns)\n",
    "\n",
    "\n",
    "# Função para prever o sentimento com TextBlob\n",
    "def predict_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    # TextBlob retorna polaridade dentro do intervalo [-1, 1], onde valores positivos indicam sentimentos positivos\n",
    "    return \"positive\" if analysis.sentiment.polarity >= 0 else \"negative\"\n",
    "\n",
    "\n",
    "# Aplicar a função de previsão de sentimento aos dados de teste\n",
    "test_data[\"predicted_sentiment_TextBlob\"] = test_data[\"review\"].apply(predict_sentiment)\n",
    "\n",
    "# Converter rótulos de string para binários\n",
    "test_data[\"sentiment\"] = test_data[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "test_data[\"predicted_sentiment_TextBlob\"] = test_data[\"predicted_sentiment_TextBlob\"].map({\"positive\": 1, \"negative\": 0})\n",
    "\n",
    "print(test_data[\"sentiment\"].value_counts())\n",
    "\n",
    "# Calcular as métricas de avaliação\n",
    "print(classification_report(test_data[\"sentiment\"], test_data[\"predicted_sentiment_TextBlob\"] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.39      0.53       741\n",
      "           1       0.78      0.96      0.86      1676\n",
      "\n",
      "    accuracy                           0.79      2417\n",
      "   macro avg       0.80      0.68      0.70      2417\n",
      "weighted avg       0.79      0.79      0.76      2417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Inicializar o analisador de sentimentos VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "# Função para prever o sentimento com VADER\n",
    "def predict_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return \"positive\" if scores[\"compound\"] >= 0 else \"negative\"\n",
    "\n",
    "\n",
    "# Aplicar a função de previsão de sentimento aos dados de teste\n",
    "test_data[\"predicted_sentiment_VADER\"] = test_data[\"review\"].apply(predict_sentiment)\n",
    "\n",
    "# Converter rótulos de string para binários\n",
    "test_data[\"predicted_sentiment_VADER\"] = test_data[\"predicted_sentiment_VADER\"].map({\"positive\": 1, \"negative\": 0})\n",
    "\n",
    "print(classification_report(test_data[\"sentiment\"], test_data[\"predicted_sentiment_VADER\"] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 01:43:52 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 167MB/s]                     \n",
      "2024-04-10 01:43:53 INFO: Downloaded file to /Users/nuno/stanza_resources/resources.json\n",
      "2024-04-10 01:43:53 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-04-10 01:43:53 INFO: Loading these models for language: en (English):\n",
      "==============================\n",
      "| Processor | Package        |\n",
      "------------------------------\n",
      "| tokenize  | combined       |\n",
      "| mwt       | combined       |\n",
      "| sentiment | sstplus_charlm |\n",
      "==============================\n",
      "\n",
      "2024-04-10 01:43:53 INFO: Using device: cpu\n",
      "2024-04-10 01:43:53 INFO: Loading: tokenize\n",
      "2024-04-10 01:43:54 INFO: Loading: mwt\n",
      "2024-04-10 01:43:54 INFO: Loading: sentiment\n",
      "2024-04-10 01:43:54 INFO: Done loading processors!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m avg_sentiment \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Aplicar a função de previsão de sentimento aos dados de teste\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_sentiment_Stanza\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_sentiment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Converter rótulos de string para binários\u001b[39;00m\n\u001b[1;32m     21\u001b[0m test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_sentiment_Stanza\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_sentiment_Stanza\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     22\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4791\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m, in \u001b[0;36mpredict_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_sentiment\u001b[39m(text):\n\u001b[0;32m---> 11\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     sentiment_scores \u001b[38;5;241m=\u001b[39m [sentence\u001b[38;5;241m.\u001b[39msentiment \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msentences]\n\u001b[1;32m     13\u001b[0m     avg_sentiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(sentiment_scores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentiment_scores)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/stanza/pipeline/core.py:477\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/stanza/pipeline/core.py:428\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[1;32m    427\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[0;32m--> 428\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/stanza/pipeline/sentiment_processor.py:66\u001b[0m, in \u001b[0;36mSentimentProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     64\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mextract_sentences(document)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 66\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# TODO: allow a classifier processor for any attribute, not just sentiment\u001b[39;00m\n\u001b[1;32m     68\u001b[0m document\u001b[38;5;241m.\u001b[39mset(SENTIMENT, labels, to_sentence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/stanza/models/classifiers/base_classifier.py:53\u001b[0m, in \u001b[0;36mBaseClassifier.label_sentences\u001b[0;34m(self, sentences, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interval[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m interval[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# this can happen for empty text\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m labels\u001b[38;5;241m.\u001b[39mextend(predicted\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/stanza/models/classifiers/cnn_classifier.py:440\u001b[0m, in \u001b[0;36mCNNClassifier.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    437\u001b[0m     all_inputs \u001b[38;5;241m=\u001b[39m [input_vectors]\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_charlm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     char_reps_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_char_reps\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_phrase_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_charlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmodel_forward_projection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin_paddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     all_inputs\u001b[38;5;241m.\u001b[39mappend(char_reps_forward)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_charlm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/stanza/models/classifiers/cnn_classifier.py:305\u001b[0m, in \u001b[0;36mCNNClassifier.build_char_reps\u001b[0;34m(self, inputs, max_phrase_len, charlm, projection, begin_paddings, device)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_char_reps\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, max_phrase_len, charlm, projection, begin_paddings, device):\n\u001b[0;32m--> 305\u001b[0m     char_reps \u001b[38;5;241m=\u001b[39m \u001b[43mcharlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_char_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m projection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m         char_reps \u001b[38;5;241m=\u001b[39m [projection(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m char_reps]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/stanza/models/common/char_model.py:222\u001b[0m, in \u001b[0;36mCharacterLanguageModel.build_char_representation\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    219\u001b[0m chars \u001b[38;5;241m=\u001b[39m get_long_tensor(chars, \u001b[38;5;28mlen\u001b[39m(all_data), pad_id\u001b[38;5;241m=\u001b[39mvocab\u001b[38;5;241m.\u001b[39munit2id(CHARLM_END))\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 222\u001b[0m     output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     res \u001b[38;5;241m=\u001b[39m [output[i, offsets] \u001b[38;5;28;01mfor\u001b[39;00m i, offsets \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(char_offsets)]\n\u001b[1;32m    224\u001b[0m     res \u001b[38;5;241m=\u001b[39m unsort(res, orig_idx)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/stanza/models/common/char_model.py:153\u001b[0m, in \u001b[0;36mCharacterLanguageModel.forward\u001b[0;34m(self, chars, charlens, hidden)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[1;32m    151\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_h_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous(),\n\u001b[1;32m    152\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_c_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[0;32m--> 153\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pad_packed_sequence(output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/stanza/models/common/packed_lstm.py:22\u001b[0m, in \u001b[0;36mPackedLSTM.forward\u001b[0;34m(self, input, lengths, hx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, PackedSequence):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m pack_padded_sequence(\u001b[38;5;28minput\u001b[39m, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad:\n\u001b[1;32m     24\u001b[0m     res \u001b[38;5;241m=\u001b[39m (pad_packed_sequence(res[\u001b[38;5;241m0\u001b[39m], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)[\u001b[38;5;241m0\u001b[39m], res[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/rnn.py:881\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    878\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    879\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 881\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    884\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Inicializar o pipeline NLP Stanza\n",
    "nlp = stanza.Pipeline(\"en\", processors=\"tokenize,sentiment\")\n",
    "\n",
    "\n",
    "# Função para prever o sentimento com Stanza\n",
    "def predict_sentiment(text):\n",
    "    doc = nlp(text)\n",
    "    sentiment_scores = [sentence.sentiment for sentence in doc.sentences]\n",
    "    avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
    "    return \"positive\" if avg_sentiment >= 1 else \"negative\"\n",
    "\n",
    "\n",
    "# Aplicar a função de previsão de sentimento aos dados de teste\n",
    "test_data[\"predicted_sentiment_Stanza\"] = test_data[\"review\"].apply(predict_sentiment)\n",
    "\n",
    "# Converter rótulos de string para binários\n",
    "test_data[\"predicted_sentiment_Stanza\"] = test_data[\"predicted_sentiment_Stanza\"].map(\n",
    "    {\"positive\": 1, \"negative\": 0}\n",
    ")\n",
    "\n",
    "# Calcular as métricas de avaliação\n",
    "print(classification_report(test_data[\"sentiment\"], test_data[\"predicted_sentiment_Stanza\"] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preparação de dados e aplicação de um léxico de sentimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Léxicos de polaridade** são recursos valiosos em processamento de linguagem natural (NLP), particularmente úteis para tarefas como análise de sentimentos, onde o objetivo é determinar a atitude ou emoção expressa em um texto. Esses léxicos consistem em listas de palavras, cada uma associada a uma pontuação ou etiqueta que indica se a palavra tem uma conotação positiva, negativa ou neutra. Alguns léxicos também incluem intensidades para refletir o grau de emoção. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NRC Word-Emotion Association Lexicon (EmoLex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English Word</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aback</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abacus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abatement</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abba</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abbot</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abbreviate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English Word  negative  positive\n",
       "0        aback         0         0\n",
       "1       abacus         0         0\n",
       "2      abandon         1         0\n",
       "3    abandoned         1         0\n",
       "4  abandonment         1         0\n",
       "5        abate         0         0\n",
       "6    abatement         0         0\n",
       "7         abba         0         1\n",
       "8        abbot         0         0\n",
       "9   abbreviate         0         0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista de palavras inglesas e o seu respetivo sentimento (positivo ou negativo)\n",
    "\n",
    "emolex = pd.read_csv(\n",
    "    \"NCR-lexicon.txt\",\n",
    "    skiprows=0,\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "\n",
    "emolex = emolex[[\"English Word\", \"negative\", \"positive\"]]\n",
    "\n",
    "emolex_dict = {row[\"English Word\"]: (row[\"positive\"], row[\"negative\"]) for index, row in emolex.iterrows()}\n",
    "\n",
    "emolex.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emolex_dict = {row[\"English Word\"]: (row[\"positive\"], row[\"negative\"]) for index, row in emolex.iterrows()}\n",
    "\n",
    "emolex_dict['happy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Afinn Lexicon ()\n",
    "\n",
    "Este léxico atribui a cada palavra uma pontuação de -5 a 5, indicando a intensidade do sentimento negativo ou positivo. É útil em contextos onde a intensidade precisa ser medida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandons</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abducted</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abduction</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abductions</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abhor</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abhorred</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abhorrent</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abhors</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  value\n",
       "0     abandon     -2\n",
       "1   abandoned     -2\n",
       "2    abandons     -2\n",
       "3    abducted     -2\n",
       "4   abduction     -2\n",
       "5  abductions     -2\n",
       "6       abhor     -3\n",
       "7    abhorred     -3\n",
       "8   abhorrent     -3\n",
       "9      abhors     -3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afinn = pd.read_csv(\"Afinn.csv\", encoding=\"latin1\")\n",
    "\n",
    "afinn.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afinn_dict = {row[\"word\"]: row[\"value\"] for index, row in afinn.iterrows()}\n",
    "\n",
    "afinn_dict['happy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bing Lexicon ()\n",
    "\n",
    "Este léxico é frequentemente usado em análise de sentimentos para identificar e contar o número de palavras positivas e negativas em um texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-faces</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abnormal</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abolish</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abominable</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abominably</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abominate</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abomination</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abort</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aborted</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aborts</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word sentiment\n",
       "0      2-faces  negative\n",
       "1     abnormal  negative\n",
       "2      abolish  negative\n",
       "3   abominable  negative\n",
       "4   abominably  negative\n",
       "5    abominate  negative\n",
       "6  abomination  negative\n",
       "7        abort  negative\n",
       "8      aborted  negative\n",
       "9       aborts  negative"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bing = pd.read_csv(\"Bing.csv\")\n",
    "\n",
    "bing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adiciona colunas binárias para os sentimentos\n",
    "bing['negative'] = (bing['sentiment'] == 'negative').astype(int)\n",
    "bing['positive'] = (bing['sentiment'] == 'positive').astype(int)\n",
    "\n",
    "# Remove a coluna de sentimento\n",
    "bing = bing.drop('sentiment', axis=1)\n",
    "\n",
    "#bing.head(10)\n",
    "\n",
    "bing_dict = {row[\"word\"]: (row[\"positive\"], row[\"negative\"]) for index, row in bing.iterrows()}\n",
    "\n",
    "bing_dict['happy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Limpeza do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try and except: foi utilizado em quase todas as funções porque mais para a frente vamos fazer as combinações e por isso não vamos precisar sempre de tokenizar pois vai bastar uma vez, então se der erro ele apenas passa o text para o words. O words essencialmente nesses casos vai ser já a lista de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Função de limpeza de texto\n",
    "def clean(text):\n",
    "    \n",
    "    # Transformar em minúsculas\n",
    "    text = text.lower()\n",
    "    # Remover código HTML\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "    # Remover URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https?\\:\\/\\/\\S+\", \"\", text)\n",
    "    # Remover menções a usuários (não comum em reviews da Amazon)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    # Remover hashtags (também não comum em reviews da Amazon)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    # Remover entidades HTML (&amp;, &lt;, etc.)\n",
    "    text = re.sub(r\"&\\w+;\", \"\", text)\n",
    "    # Remover números (avaliações numéricas, preços, etc.)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # Substituir caracteres de pontuação por espaços\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    # Remover espaços múltiplos e linhas novas\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Remover espaços no início e no fim\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# train_clean = train_data[\"review\"].apply(clean)\n",
    "# test_clean = test_data[\"review\"].apply(clean)\n",
    "\n",
    "\n",
    "# train_clean.to_csv(\n",
    "#    \"/Users/marianaborralho/Desktop/M. Ciencia de Dados/2 Semestre/Text Mining/Trabalho/train_clean.csv\",\n",
    "#    index=False,\n",
    "# )\n",
    "# test_clean.to_csv(\n",
    "#    \"/Users/marianaborralho/Desktop/M. Ciencia de Dados/2 Semestre/Text Mining/Trabalho/train_clean.csv\",\n",
    "#    index=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi, I have to have black tea everyday. I have done it in the past 25 years and it's a habit of mine. Ahmad tea is one of my favorite brands and I highly recommend it if you like black tea. I think this is going to remain my favorite for the next 100 years or so :)\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"review\"][157]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_clean[157]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nuno/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "# Função de tokenização\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "# train_tok = train_data[\"review\"].apply(tokenize)\n",
    "\n",
    "# test_tok = test_data[\"review\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tok[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Função que remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    \n",
    "    try:\n",
    "        words = word_tokenize(text)\n",
    "    except:\n",
    "        words = text\n",
    "        \n",
    "    words = [word for word in words if word not in nltk_stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "# train_stopwords = train_data[\"review\"].apply(remove_stopwords)\n",
    "# test_stopwords = test_data[\"review\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ordered this for my wife as it was reccomended by our daughter.  She has this almost every morning and likes all flavors.  She\\'s happy, I\\'m happy!!!<br /><a href=\"http://www.amazon.com/gp/product/B001EO5QW8\">McCANN\\'S Instant Irish Oatmeal, Variety Pack of Regular, Apples & Cinnamon, and Maple & Brown Sugar, 10-Count Boxes (Pack of 6)</a>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"review\"][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stopwords[28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Função de stemming\n",
    "def stem(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    try:\n",
    "        words = word_tokenize(text)\n",
    "    except:\n",
    "        words = text\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "# train_stem = train_data['review'].apply(stem)\n",
    "# test_stem = test_data['review'].apply(stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ordered this for my wife as it was reccomended by our daughter.  She has this almost every morning and likes all flavors.  She\\'s happy, I\\'m happy!!!<br /><a href=\"http://www.amazon.com/gp/product/B001EO5QW8\">McCANN\\'S Instant Irish Oatmeal, Variety Pack of Regular, Apples & Cinnamon, and Maple & Brown Sugar, 10-Count Boxes (Pack of 6)</a>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"review\"][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_stem[28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Função de lemmatization\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    try:\n",
    "        words = word_tokenize(text)\n",
    "    except:\n",
    "        words = text\n",
    "        \n",
    "    words = [lemmatizer.lemmatize(word, \"v\") for word in words]\n",
    "    return words\n",
    "\n",
    "# train_lemmatized = train_data[\"review\"].apply(lemmatize)\n",
    "# test_lemmatized = test_data[\"review\"].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ordered this for my wife as it was reccomended by our daughter.  She has this almost every morning and likes all flavors.  She\\'s happy, I\\'m happy!!!<br /><a href=\"http://www.amazon.com/gp/product/B001EO5QW8\">McCANN\\'S Instant Irish Oatmeal, Variety Pack of Regular, Apples & Cinnamon, and Maple & Brown Sugar, 10-Count Boxes (Pack of 6)</a>'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"review\"][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_lemmatized[28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Contrações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "# Função que trata das contrações\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "# train_expanded = train_data[\"review\"].apply(expand_contractions)\n",
    "# test_expanded = test_data[\"review\"].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ordered this for my wife as it was reccomended by our daughter.  She has this almost every morning and likes all flavors.  She\\'s happy, I\\'m happy!!!<br /><a href=\"http://www.amazon.com/gp/product/B001EO5QW8\">McCANN\\'S Instant Irish Oatmeal, Variety Pack of Regular, Apples & Cinnamon, and Maple & Brown Sugar, 10-Count Boxes (Pack of 6)</a>'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"review\"][28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_expanded[28]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- POS tagging (Part-of-Speech tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foram removidas 3 classes: Preposições, Conjunções e Pronomes\n",
    "\n",
    "Nota: Algumas destas classes podem ser removidas usando a função de stopwords (são muito comuns e geralmente não contribuem muito para o significado de uma frase)\n",
    "\n",
    "Ver se faz sentido adicionar/retirar alguma classe\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        words = word_tokenize(text)\n",
    "    except:\n",
    "        words = text\n",
    "    \n",
    "    # Aplica o POS tagging a cada palavra\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    # Filtra palavras que são preposições (IN), conjunções (CC) ou pronomes (PRP, PRP$)\n",
    "    filtered_words = [word for word, tag in pos_tags if tag not in ['IN', 'CC', 'PRP', 'PRP$']]\n",
    "\n",
    "    # Junta as palavras filtradas de volta em uma string\n",
    "    return filtered_words\n",
    "\n",
    "# Exemplo \n",
    "#text_example = \"The quick brown fox jumps over the lazy dog\"\n",
    "#filtered_text = pos_tagging(text_example)\n",
    "#print(filtered_text)\n",
    "\n",
    "# train_pos = train_data[\"review\"].apply(pos_tagging)\n",
    "# test_pos = test_data[\"review\"].apply(pos_tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The candy is just red , No flavor . Just  plan and chewy .  I would never buy them again'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['review'][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pos[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tratamento da Negação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "# Função que trata da negação\n",
    "def negation(tokens):\n",
    "    try:\n",
    "        tokens = word_tokenize(tokens)\n",
    "    except:\n",
    "        tokens = tokens\n",
    "    return mark_negation(tokens)\n",
    "\n",
    "# train_negation = train_data[\"review\"].apply(negation)\n",
    "# test_negation = test_data[\"review\"].apply(negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The candy is just red , No flavor . Just  plan and chewy .  I would never buy them again'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['review'][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_negation[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função **mark_negation** da biblioteca NLTK anexa um sufixo '_NEG' a todas as palavras que aparecem após uma palavra de negação até a próxima pontuação. As palavras de negação padrão que a função considera são 'not', 'no', 'never', 'nobody', 'none', 'nowhere', 'nothing', 'neither', 'nor', 'n', 'nt', 'n’t'. A pontuação padrão que a função considera para terminar o uso de negação é '.', ':', ';', '!', '?'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problema**: Não considera a virgula como pontuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "\n",
    "# Função que trata da negação\n",
    "def mark_negation_custom(text):\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    negation_words = ['not', 'no', 'never', 'nobody', 'none', 'nowhere', 'nothing', 'neither', 'nor', 'n', 'nt']\n",
    "    punctuation = ['.', ':', ';', '!', '?', ',']  \n",
    "    suffixed = []\n",
    "    neg_scope = False\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in negation_words:\n",
    "            neg_scope = True\n",
    "            suffixed.append(token)\n",
    "            continue\n",
    "\n",
    "        if token in punctuation:\n",
    "            neg_scope = False\n",
    "\n",
    "        if neg_scope:\n",
    "            token = \"NOT_\" + token \n",
    "\n",
    "        suffixed.append(token)\n",
    "\n",
    "    return suffixed\n",
    "\n",
    "# train_negation_custom = train_data[\"review\"].apply(mark_negation_custom)\n",
    "# test_negation_custom = test_data[\"review\"].apply(mark_negation_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"review\"][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_negation_custom[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classificação final a atribuir ao texto\n",
    "\n",
    "**ideia geral:** quantificar quantas palavras do texto a analisar estão classificadas como Positive e Negative e, consoante a classe mais frequente, decidir qual a classificação final a atribuír ao texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def count_sentiment_words(tokens, lexicon):\n",
    "\n",
    "    # Inicializa contadores\n",
    "    counts = {\"positive\": 0, \"negative\": 0}\n",
    "    counter_not_found = 0\n",
    "    not_found_words = []  # Lista para armazenar palavras não encontradas no léxico\n",
    "    last_sentiment = None  # Acompanha o último sentimento encontrado\n",
    "    \n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in lexicon:\n",
    "            positive_score, negative_score = lexicon[token]\n",
    "            counts[\"positive\"] += positive_score\n",
    "            counts[\"negative\"] += negative_score\n",
    "            \n",
    "            # Atualiza o último sentimento observado\n",
    "            if positive_score > negative_score:\n",
    "                last_sentiment = \"positive\"\n",
    "            elif negative_score > positive_score:\n",
    "                last_sentiment = \"negative\"\n",
    "        elif '_NEG' in token:\n",
    "            counts[\"negative\"] += 3\n",
    "            last_sentiment = \"negative\"\n",
    "            \n",
    "        else:\n",
    "            counter_not_found += 1\n",
    "            not_found_words.append(token)  # Adiciona a palavra à lista\n",
    "            \n",
    "    total_tokens = len(tokens)\n",
    "    not_found_percentage = (counter_not_found / total_tokens) * 100\n",
    "    \n",
    "    # Determina o sentimento geral\n",
    "    if counts[\"positive\"] > counts[\"negative\"]:\n",
    "        sentiment = \"positive\"\n",
    "    elif counts[\"negative\"] > counts[\"positive\"]:\n",
    "        sentiment = \"negative\"\n",
    "    else:\n",
    "        # Estratégia para desempate: usa o sentimento da última palavra com sentimento\n",
    "        if last_sentiment:\n",
    "            sentiment = last_sentiment\n",
    "        else:\n",
    "            # Se não houver palavras com sentimento ou se precisar de uma estratégia padrão\n",
    "            sentiment = random.choice([\"positive\", \"negative\"])  # Ou outra estratégia padrão\n",
    "    \n",
    "    return sentiment, not_found_words, not_found_percentage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentiment_words_Afinn(tokens, lexicon):\n",
    "    \n",
    "    counts = 0\n",
    "    counter_not_found = 0\n",
    "    not_found_words = []  # Lista para armazenar palavras não encontradas no léxico\n",
    "    last_sentiment = None  # Acompanha o último sentimento encontrado\n",
    "    \n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in lexicon:\n",
    "            score = lexicon[token]\n",
    "            counts += score\n",
    "            \n",
    "            # Atualiza o último sentimento observado\n",
    "            if score > 0:\n",
    "                last_sentiment = \"positive\"\n",
    "            elif score < 0:\n",
    "                last_sentiment = \"negative\"\n",
    "        elif '_NEG' in token:\n",
    "            counts -= 5\n",
    "            last_sentiment = \"negative\"    \n",
    "        else:\n",
    "            counter_not_found += 1\n",
    "            not_found_words.append(token)  # Adiciona a palavra à lista de não encontradas\n",
    "            \n",
    "    total_tokens = len(tokens)\n",
    "    not_found_percentage = (counter_not_found / total_tokens) * 100\n",
    "    \n",
    "    # Determina o sentimento geral\n",
    "    if counts > 0:\n",
    "        sentiment = \"positive\"\n",
    "    elif counts < 0:\n",
    "        sentiment = \"negative\"\n",
    "    else:\n",
    "        # Estratégia de desempate: usa o último sentimento observado\n",
    "        if last_sentiment:\n",
    "            sentiment = last_sentiment\n",
    "        else:\n",
    "            # Se não houve palavras com sentimento detectado, escolhe aleatoriamente\n",
    "            sentiment = random.choice([\"positive\", \"negative\"])  # Ou outra estratégia padrão\n",
    "    \n",
    "    return sentiment, not_found_words, not_found_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(data, actual_col, predicted_col, not_found_col):\n",
    "\n",
    "    correct_predictions = data[data[actual_col] == data[predicted_col]]\n",
    "    accuracy = len(correct_predictions) / len(data) * 100\n",
    "    not_found_average = data[not_found_col].mean()  # Calcula a média de percentagens de palavras não encontradas\n",
    "    return accuracy, not_found_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideia geral: queremos testar todas as combinações quer seja com os diferentes lexicos como os metodos de processamento. Clean e Expand Contractions serão sempre os primeiros metodos a ser aplicados pois não é preciso ser tokenizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Diferentes combinacoes de pre process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   0%|          | 0/282 [00:00<?, ?combo/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   0%|          | 1/282 [00:00<02:08,  2.19combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_predicted\n",
      "positive    2058\n",
      "negative     359\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   1%|          | 2/282 [00:00<02:00,  2.33combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_predicted\n",
      "positive    1937\n",
      "negative     480\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   1%|          | 3/282 [00:01<01:57,  2.38combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_predicted\n",
      "positive    1967\n",
      "negative     450\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   1%|▏         | 4/282 [00:01<02:15,  2.04combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_predicted\n",
      "positive    2064\n",
      "negative     353\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   2%|▏         | 5/282 [00:02<02:09,  2.14combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_predicted\n",
      "positive    1930\n",
      "negative     487\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   2%|▏         | 6/282 [00:02<02:02,  2.25combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_predicted\n",
      "positive    1966\n",
      "negative     451\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   2%|▏         | 7/282 [00:03<02:08,  2.13combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_predicted\n",
      "positive    2022\n",
      "negative     395\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   3%|▎         | 8/282 [00:03<02:13,  2.05combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_predicted\n",
      "positive    1881\n",
      "negative     536\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   3%|▎         | 9/282 [00:04<02:27,  1.85combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_predicted\n",
      "positive    1887\n",
      "negative     530\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   4%|▎         | 10/282 [00:04<02:25,  1.86combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_predicted\n",
      "positive    2018\n",
      "negative     399\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   4%|▍         | 11/282 [00:05<02:24,  1.87combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_predicted\n",
      "positive    1894\n",
      "negative     523\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   4%|▍         | 12/282 [00:06<02:25,  1.85combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_predicted\n",
      "positive    1881\n",
      "negative     536\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   5%|▍         | 13/282 [00:06<02:28,  1.81combo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_predicted\n",
      "negative    1624\n",
      "positive     793\n",
      "Name: count, dtype: int64\n",
      "sentiment\n",
      "positive    1676\n",
      "negative     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing combinations:   5%|▍         | 13/282 [00:07<02:29,  1.79combo/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 91\u001b[0m\n\u001b[1;32m     84\u001b[0m         combo_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_predicted\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_words\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot_found_percentage\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m combo_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     85\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m review: count_sentiment_words(word_tokenize(review), lexicon)\n\u001b[1;32m     86\u001b[0m         )\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mSeries)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m         combo_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_predicted\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_words\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot_found_percentage\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mcombo_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreview\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount_sentiment_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlexicon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 91\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m accuracy, not_found_average \u001b[38;5;241m=\u001b[39m calculate_accuracy(combo_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_predicted\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot_found_percentage\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(combo_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_predicted\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4791\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/apply.py:1514\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m-> 1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_expanddim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmapped\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor(mapped, index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   1517\u001b[0m         obj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1518\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:840\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    839\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 840\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    849\u001b[0m         arrays,\n\u001b[1;32m    850\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    853\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    854\u001b[0m     )\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:839\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    837\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m _list_of_dict_to_arrays(data, columns)\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m--> 839\u001b[0m     arr, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_list_of_series_to_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;66;03m# last ditch effort\u001b[39;00m\n\u001b[1;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/internals/construction.py:882\u001b[0m, in \u001b[0;36m_list_of_series_to_arrays\u001b[0;34m(data, columns)\u001b[0m\n\u001b[1;32m    880\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer_cache[\u001b[38;5;28mid\u001b[39m(index)]\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer_cache[\u001b[38;5;28mid\u001b[39m(index)] \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    884\u001b[0m values \u001b[38;5;241m=\u001b[39m extract_array(s, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    885\u001b[0m aligned_values\u001b[38;5;241m.\u001b[39mappend(algorithms\u001b[38;5;241m.\u001b[39mtake_nd(values, indexer))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3932\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3928\u001b[0m         indexer[mask] \u001b[38;5;241m=\u001b[39m loc\n\u001b[1;32m   3930\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_platform_int(indexer)\n\u001b[0;32m-> 3932\u001b[0m pself, ptarget \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_downcast_for_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pself \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m ptarget \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m target:\n\u001b[1;32m   3934\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pself\u001b[38;5;241m.\u001b[39mget_indexer(\n\u001b[1;32m   3935\u001b[0m         ptarget, method\u001b[38;5;241m=\u001b[39mmethod, limit\u001b[38;5;241m=\u001b[39mlimit, tolerance\u001b[38;5;241m=\u001b[39mtolerance\n\u001b[1;32m   3936\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:6364\u001b[0m, in \u001b[0;36mIndex._maybe_downcast_for_indexing\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   6360\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   6361\u001b[0m         \u001b[38;5;66;03m# let's instead try with a straight Index\u001b[39;00m\n\u001b[1;32m   6362\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Index(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values)\n\u001b[0;32m-> 6364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_object_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m is_object_dtype(other\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   6365\u001b[0m     \u001b[38;5;66;03m# Reverse op so we dont need to re-implement on the subclasses\u001b[39;00m\n\u001b[1;32m   6366\u001b[0m     other, \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39m_maybe_downcast_for_indexing(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   6368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m, other\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/dtypes/common.py:165\u001b[0m, in \u001b[0;36mis_object_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_object_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    Check whether an array-like or dtype is of the object dtype.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _is_dtype_type(arr_or_dtype, \u001b[43mclasses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject_\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/dtypes/common.py:121\u001b[0m, in \u001b[0;36mclasses\u001b[0;34m(*klasses)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_value\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclasses\u001b[39m(\u001b[38;5;241m*\u001b[39mklasses) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable:\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate if the tipo is a subclass of the klasses.\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m tipo: \u001b[38;5;28missubclass\u001b[39m(tipo, klasses)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm # Barra de progresso\n",
    "\n",
    "# Lista de métodos de processamento de texto\n",
    "methods = [clean, expand_contractions, negation, remove_stopwords, lemmatize, stem]\n",
    "# Dicionário de léxicos\n",
    "lexicon_names = {'afinn_dict': afinn_dict, 'bing_dict': bing_dict, 'emolex_dict': emolex_dict}\n",
    "\n",
    "final_combinations = []\n",
    "\n",
    "# Combina métodos individualmete\n",
    "for method in methods:\n",
    "    final_combinations.append((method,))\n",
    "\n",
    "# Combina 2 métodos\n",
    "for (method1, method2) in itertools.combinations(methods, 2):\n",
    "    final_combinations.append((method1, method2))\n",
    "\n",
    "# Combina 3 métodos\n",
    "for combination in itertools.combinations(methods, 3):\n",
    "    final_combinations.append(combination)\n",
    "\n",
    "# Combina 4 métodos\n",
    "for combination in itertools.combinations(methods, 4):\n",
    "    final_combinations.append(combination)\n",
    "\n",
    "# Combina todos os métodos\n",
    "for combination in itertools.combinations(methods, 5):\n",
    "    final_combinations.append(combination)\n",
    "\n",
    "\n",
    "# Verifica se os métodos 'clean' e 'expand_contractions'estão na ordem correta quando aplicável e evita que incluam combinações com \n",
    "# lemmatize e stem em simultâneo\n",
    "new_combinations = []\n",
    "for combination in final_combinations:\n",
    "    if clean in combination and expand_contractions in combination or clean in combination or expand_contractions in combination:\n",
    "        if clean in combination and combination.index(clean) == 0:\n",
    "            new_combinations.append(combination)\n",
    "        elif expand_contractions in combination and combination.index(expand_contractions) == 0:\n",
    "            new_combinations.append(combination)\n",
    "        elif clean in combination and expand_contractions in combination and combination.index(clean) == 0 and combination.index(expand_contractions) == 1:\n",
    "            new_combinations.append(combination)\n",
    "        else:\n",
    "            continue\n",
    "    if lemmatize in combination and stem in combination:\n",
    "        continue\n",
    "    else:\n",
    "        new_combinations.append(combination)\n",
    "\n",
    "final_combinations = new_combinations\n",
    "        \n",
    "        \n",
    "lexicons = list(lexicon_names.keys())\n",
    "final_combinations = list(itertools.product(final_combinations, lexicons))\n",
    "\n",
    "results = []\n",
    "\n",
    "for combo in tqdm(final_combinations, desc='Processing combinations', unit='combo'):\n",
    "\n",
    "    combo_data = test_data.copy() # Cópia dos dados de teste\n",
    "    # Aplica métodos de processamento de texto\n",
    "    text_processing_methods = combo[0]\n",
    "\n",
    "    for method in text_processing_methods:\n",
    "        combo_data['review'] = combo_data['review'].apply(method)\n",
    "\n",
    "    # Aplica o léxico escolhido\n",
    "    lexicon_name = combo[1]\n",
    "    lexicon = lexicon_names[lexicon_name]\n",
    "    \n",
    "    if lexicon == afinn_dict:\n",
    "        # Função específica para AFINN\n",
    "        if text_processing_methods == (clean, expand_contractions) or text_processing_methods == (clean,) or text_processing_methods == (expand_contractions,):\n",
    "            combo_data[['sentiment_predicted', 'missing_words', 'not_found_percentage']] = combo_data['review'].apply(\n",
    "                lambda review: count_sentiment_words_Afinn(word_tokenize(review), lexicon)\n",
    "            ).apply(pd.Series)\n",
    "        else:\n",
    "            combo_data[['sentiment_predicted', 'missing_words', 'not_found_percentage']] = combo_data['review'].apply(\n",
    "                lambda review: count_sentiment_words_Afinn(review, lexicon)\n",
    "            ).apply(pd.Series)\n",
    "    else:\n",
    "        # Função geral para outros léxicos\n",
    "        if text_processing_methods == (clean, expand_contractions) or text_processing_methods == (clean,) or text_processing_methods == (expand_contractions,):\n",
    "            combo_data[['sentiment_predicted', 'missing_words', 'not_found_percentage']] = combo_data['review'].apply(\n",
    "                lambda review: count_sentiment_words(word_tokenize(review), lexicon)\n",
    "            ).apply(pd.Series)\n",
    "    \n",
    "        else:\n",
    "            combo_data[['sentiment_predicted', 'missing_words', 'not_found_percentage']] = combo_data['review'].apply(\n",
    "                lambda review: count_sentiment_words(review, lexicon)\n",
    "            ).apply(pd.Series)\n",
    "\n",
    "        \n",
    "    accuracy, not_found_average = calculate_accuracy(combo_data, 'sentiment', 'sentiment_predicted', 'not_found_percentage')\n",
    "    \n",
    "    report_dict = classification_report(combo_data['sentiment'], combo_data['sentiment_predicted'], output_dict=True)\n",
    "    \n",
    "    # Processa métricas do relatório de classificação\n",
    "    report_metrics = {\n",
    "        f\"{label}_{metric}\": report_dict[label][metric]\n",
    "        for label in report_dict if label not in ['accuracy', 'macro avg', 'weighted avg']\n",
    "        for metric in ['precision', 'recall', 'f1-score', 'support']\n",
    "    }\n",
    "\n",
    "     # Combina métricas do relatório de classificação com outros resultados e adicionar à lista de resultados\n",
    "    results.append({\n",
    "        'combination': ([method.__name__ for method in text_processing_methods], lexicon_name),\n",
    "        'accuracy': accuracy,\n",
    "        'not_found_average': not_found_average,\n",
    "        **report_metrics  # Desempacota as métricas do relatório de classificação\n",
    "    })\n",
    "    \n",
    "# Converte resultados em DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Guarda os resultados em um arquivo CSV\n",
    "results_df.to_csv(\"text_processing_combinations_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average percentage of not found words: 64.67413768869916%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      0.32      0.38       741\n",
      "    positive       0.74      0.85      0.79      1676\n",
      "\n",
      "    accuracy                           0.69      2417\n",
      "   macro avg       0.61      0.58      0.59      2417\n",
      "weighted avg       0.66      0.69      0.67      2417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se quisermos executar algum método individualmente temos esta forma   \n",
    "\n",
    "test_data[['sentiment_predicted_tokenized_cleaned_expand_remove_stem', 'missing_words', 'not_found_percentage']] = test_data[\"review\"].apply(lambda review: count_sentiment_words(lemmatize(pos_tagging(expand_contractions(clean(review)))), emolex_dict)).apply(pd.Series)\n",
    "accuracy, not_found_average = calculate_accuracy(test_data, 'sentiment', 'sentiment_predicted_tokenized_cleaned_expand_remove_stem', 'not_found_percentage')\n",
    "print(f\"Average percentage of not found words: {not_found_average}%\")\n",
    "\n",
    "\n",
    "print(classification_report(test_data[\"sentiment\"], test_data[\"sentiment_predicted_tokenized_cleaned_expand_remove_stem\"] ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average percentage of not found words: 92.42641165899917%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.33      0.45       741\n",
      "    positive       0.76      0.94      0.84      1676\n",
      "\n",
      "    accuracy                           0.75      2417\n",
      "   macro avg       0.73      0.63      0.64      2417\n",
      "weighted avg       0.74      0.75      0.72      2417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Se quisermos executar algum método individualmente temos esta forma   \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "test_data[['sentiment_predicted_tokenized_cleaned', 'missing_words', 'not_found_percentage']] = test_data[\"review\"].apply(lambda review: count_sentiment_words_Afinn(word_tokenize(clean(review)), afinn_dict)).apply(pd.Series)\n",
    "accuracy, not_found_average = calculate_accuracy(test_data, 'sentiment', 'sentiment_predicted_tokenized_cleaned', 'not_found_percentage')\n",
    "print(f\"Average percentage of not found words: {not_found_average}%\")\n",
    "\n",
    "\n",
    "print(classification_report(test_data[\"sentiment\"], test_data[\"sentiment_predicted_tokenized_cleaned\"] ))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average percentage of not found words: 91.06107617655998%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.44      0.53       741\n",
      "    positive       0.78      0.90      0.84      1676\n",
      "\n",
      "    accuracy                           0.76      2417\n",
      "   macro avg       0.73      0.67      0.69      2417\n",
      "weighted avg       0.75      0.76      0.75      2417\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "test_data[['sentiment_predicted_tokenized_cleaned', 'missing_words', 'not_found_percentage']] = test_data[\"review\"].apply(lambda review: count_sentiment_words(word_tokenize(clean(review)), bing_dict)).apply(pd.Series)\n",
    "accuracy, not_found_average = calculate_accuracy(test_data, 'sentiment', 'sentiment_predicted_tokenized_cleaned', 'not_found_percentage')\n",
    "print(f\"Average percentage of not found words: {not_found_average}%\")\n",
    "\n",
    "\n",
    "print(classification_report(test_data[\"sentiment\"], test_data[\"sentiment_predicted_tokenized_cleaned\"] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que falta para ficar finalizado ate ao ponto 1.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Léxicos: procurar alguma bibliografia que reporte a utilização de léxicos de polaridade por forma a decidir qual a melhor abordagem\n",
    "- Fazer alguma coisa em relacao aos tokens que nao sao encontrados dentro do dicionario \n",
    "- Experimentar as diferentes combinacoes de pre process text (stemming, lemmatization, stopwords, etc) \n",
    "(Juntar resultados num dataframe e exportar para csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Outra tarefa de classificação de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IMDB Movie Reviews\n",
    "\n",
    "https://www.kaggle.com/code/shubhamptrivedi/sentiment-analysis-on-imdb-movie-reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_imdb = pd.read_csv(\"/Users/marianaborralho/Desktop/M. Ciencia de Dados/2 Semestre/Text Mining/Trabalho/IMDB Dataset.csv\")\n",
    "\n",
    "data_imdb.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values: 0\n",
      "NA values: False\n"
     ]
    }
   ],
   "source": [
    "# Verificar os NA no conjunto de treino\n",
    "\n",
    "print(\"Null values:\",data_imdb.isnull().values.sum())\n",
    "print(\"NA values:\", data_imdb.isna().values.any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão em conjunto de treino e teste\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_imdb['review']\n",
    "y = data_imdb['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in training set: 40000\n",
      "Number of documents in test set: 10000\n",
      "\n",
      "Label distribution in training set:\n",
      "negative    20039\n",
      "positive    19961\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Label distribution in test set:\n",
      "positive    5039\n",
      "negative    4961\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Número de documentos\n",
    "num_docs_train = len(X_train)\n",
    "num_docs_test = len(X_test)\n",
    "\n",
    "print(f\"Number of documents in training set: {num_docs_train}\")\n",
    "print(f\"Number of documents in test set: {num_docs_test}\")\n",
    "\n",
    "# Distribuição de etiquetas\n",
    "label_distribution_train = y_train.value_counts()\n",
    "label_distribution_test = y_test.value_counts()\n",
    "\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "print(label_distribution_train)\n",
    "\n",
    "print(\"\\nLabel distribution in test set:\")\n",
    "print(label_distribution_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = sns.color_palette('deep')\n",
    "\n",
    "plt.figure(figsize=(6,4), tight_layout=True)\n",
    "plt.bar(x=['Positive', 'Negative'],\n",
    "        height=y_train.value_counts(),\n",
    "        color=colors[:2])\n",
    "plt.title('Sentiment in Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.8488\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.88      0.85      4961\n",
      "    positive       0.87      0.82      0.85      5039\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define o pipeline com vetorização de palavras e classificador Naive Bayes\n",
    "baseline_clf = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Treina o classificador baseline\n",
    "baseline_clf.fit(X_train, y_train)\n",
    "\n",
    "# Faz previsões no conjunto de teste\n",
    "baseline_predictions = baseline_clf.predict(X_test)\n",
    "\n",
    "# Relatório de classificação\n",
    "print(classification_report(y_test, baseline_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Treino de um modelo (aprendizagem automática)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Treino de um modelo (aprendizagem automática)\n",
    "\n",
    "Nesta seção, abordaremos o processo de treinar um modelo de aprendizagem automática com o objetivo de melhorar o desempenho da classificação de dados. Seguiremos os seguintes passos:\n",
    "\n",
    "## 4.1 Treinamento do Modelo\n",
    "\n",
    "1. Selecione uma ferramenta de classificação adequada, como **scikit-learn**, TensorFlow, PyTorch, etc.\n",
    "2. Prepare os dados de treino, garantindo que estão devidamente limpos e formatados para o processo de treinamento.\n",
    "3. Configure e treine o modelo com o conjunto de dados de treino.\n",
    "\n",
    "    - Várias configurações podem ser testadas:\n",
    "        - Número de features.\n",
    "        - Tratamento de maiúsculas e minúsculas.\n",
    "        - Diferentes métodos de pré-processamento de texto (normalização, stemming, lematização, etc.).\n",
    "        - Inclusão ou exclusão de informações de part-of-speech tagging.\n",
    "        - Utilização de entidades nomeadas reconhecidas no texto.\n",
    "        - Incorporação de embeddings pré-treinados, como GloVe.\n",
    "        - Se houver desbalanceamento de classes usar \n",
    "\n",
    "## 4.2 Avaliação do Modelo\n",
    "\n",
    "1. Aplique o modelo treinado ao conjunto de dados de teste.\n",
    "2. Avalie o desempenho do modelo utilizando métricas adequadas, como precisão, recall, F1-score, etc.\n",
    "3. Compare os resultados obtidos com os do modelo anterior para avaliar as melhorias.\n",
    "\n",
    "## 4.3 Documentação e Experimentação\n",
    "\n",
    "- Realize experimentos variados para entender o impacto de diferentes features e técnicas de pré-processamento.\n",
    "- Documente cada experimento, incluindo a configuração usada e os resultados obtidos.\n",
    "- Registre observações e conclusões relevantes a cada experimento.\n",
    "\n",
    "## 4.4 Referências\n",
    "\n",
    "- Alguns links que podem ajudar nesta tarefa \n",
    "    - https://www.kaggle.com/code/benroshan/sentiment-analysis-amazon-reviews#Extracting-Features-from-Cleaned-reviews\n",
    "    - https://github.com/jesseqzhen/NLP_Sentiment_Analysis\n",
    "    - https://www.kaggle.com/code/yacharki/binary-classification-amazon-reviews-84-lstm\n",
    "    - https://www.kaggle.com/code/mammadabbasli/amazon-reviews-analysis-logisticregression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Utilização de transformadores para classificação\n",
    "\n",
    "- https://medium.com/@minamehdinia213/fine-tunning-bert-model-for-amazon-product-review-and-deploying-it-into-hugging-face-model-hub-7d259839d556\n",
    "\n",
    "A aplicação de modelos pré-treinados baseados em transformadores representa uma técnica poderosa no campo da aprendizagem automática, especialmente para tarefas de classificação de texto. O processo será dividido em duas etapas principais, conforme descrito a seguir:\n",
    "\n",
    "## 5.1 Experimentação com Pipelines Pré-definidos\n",
    "\n",
    "1. Escolha um ou mais modelos baseados em transformadores disponíveis, como BERT, GPT, Transformer-XL, etc.\n",
    "2. Utilize pipelines pré-definidos oferecidos por bibliotecas como Hugging Face's Transformers para aplicar rapidamente o modelo aos seus dados.\n",
    "   \n",
    "    Exemplos de comandos de pipelines podem incluir:\n",
    "    \n",
    "    ```python\n",
    "    from transformers import pipeline\n",
    "    classifier = pipeline('text-classification', model='bert-base-uncased')\n",
    "    ```\n",
    "    \n",
    "3. Avalie o desempenho desses modelos em seu conjunto de dados sem qualquer ajuste adicional, usando métricas padrão de classificação.\n",
    "\n",
    "## 5.2 Fine-tuning de um Modelo Pré-treinado\n",
    "\n",
    "1. Selecione um modelo pré-treinado apropriado para o seu conjunto de dados e a tarefa de classificação em questão.\n",
    "2. Adapte o modelo ao seu conjunto de dados específico, o que é conhecido como fine-tuning. Isso envolve o treinamento do modelo em seu conjunto de dados, ajustando os pesos do modelo pré-treinado para melhor se adequar à sua tarefa específica.\n",
    "3. Durante o fine-tuning, experimente com diferentes hiperparâmetros, como taxa de aprendizado, número de épocas, tamanho do lote e outros relevantes para o modelo escolhido.\n",
    "   \n",
    "    Exemplo de código para fine-tuning:\n",
    "    \n",
    "    ```python\n",
    "    from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    training_args = TrainingArguments(output_dir='./results', num_train_epochs=3, ...)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    ```\n",
    "    \n",
    "4. Após o fine-tuning, avalie novamente o modelo em seu conjunto de dados, usando as mesmas métricas de classificação para comparar o desempenho com o modelo antes do ajuste."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
